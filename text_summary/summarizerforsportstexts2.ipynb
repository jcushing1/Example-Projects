{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summarizer users GloVe embeddings. The necessary file glove.GB.100d.txt can be downloaded from the internet GloVe website https://nlp.stanford.edu/projects/glove/.\n",
    "\n",
    "\n",
    "I have a text document which is \"combined.txt\" This is 10 text documents from mlg.ucd.ie/datasets/bbc.html. They correspond to news articles, in this instance all about football.\n",
    "\n",
    "This summarizer uses pagerank to rank the sentences.\n",
    "\n",
    "There are numerous methods for text summarization. This works for smaller documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.read_csv('reviews.csv')\n",
    "#print(df.head())\n",
    "\n",
    "\n",
    "file=open(\"combined.txt\", 'r')\n",
    "file_contents=file.read()\n",
    "print(file_contents)\n",
    "\n",
    "sentences=[]\n",
    "for s in file_contents:\n",
    "    sentences.append(s)\n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_stopwords(sentences):                                     \n",
    "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "  return sen_new   #sen_new\n",
    "\n",
    "def process_sents(sentences):\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    return clean_sentences\n",
    "\n",
    "\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "def summarize_sents(sentences):\n",
    "    sentence_vectors = []\n",
    "    for i in sentences:\n",
    "      if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "      else:\n",
    "        v = np.zeros((100,))\n",
    "      sentence_vectors.append(v)\n",
    "\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "      for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "          sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "print(\"summary\")\n",
    "print(\"\")\n",
    "\n",
    "sn =10\n",
    "for i in range(sn):\n",
    "    print(ranked_sentences[i][1])\n",
    "\n",
    "for i in range(10):\n",
    "    print()\n",
    "    sentences = sent_tokenize(df['review'][i])\n",
    "    summarize_sents(process_sents(sentences))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
