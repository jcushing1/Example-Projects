{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1544203532788_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-johann.bcg4vx21ib2u3idgoziibehaxd.cx.internal.cloudapp.net:8088/proxy/application_1544203532788_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-johann.bcg4vx21ib2u3idgoziibehaxd.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1544203532788_0012_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "rdd1=sc.textFile(\"/tmp/master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: wasb://johannahspark-2018-12-07t17-03-49-161z@cushingb.blob.core.windows.net/tmp/master.csv\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
      "\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1056, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1047, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 921, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 824, in collect\n",
      "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: wasb://johannahspark-2018-12-07t17-03-49-161z@cushingb.blob.core.windows.net/tmp/master.csv\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
      "\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
